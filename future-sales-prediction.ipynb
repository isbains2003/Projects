{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# importing files and libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom scipy.stats import rankdata\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom category_encoders import TargetEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nimport re\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import regexp_tokenize\nsales_train = '../input/competitive-data-science-predict-future-sales/sales_train.csv'\ntest = '../input/competitive-data-science-predict-future-sales/test.csv'\nsample_submission = '../input/competitive-data-science-predict-future-sales/sample_submission.csv'\nitem_categories = '../input/competitive-data-science-predict-future-sales/item_categories.csv'\nitems = '../input/competitive-data-science-predict-future-sales/items.csv'\nshops = '../input/competitive-data-science-predict-future-sales/shops.csv'\nfrom datetime import date\n\nimport gc\nimport pickle\n\nimport holidays\nimport sys\ntrain_data = pd.read_csv (sales_train)\ntest_data = pd.read_csv (test)\nitems_data = pd.read_csv (items)\nshops_data = pd.read_csv (shops)\nitem_cat_data = pd.read_csv (item_categories)\nsubmission_file = pd.read_csv (sample_submission)\ntest_data = test_data.drop ('ID', axis = 1)\nprint (train_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data [train_data.duplicated () == False]\ntrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\ntrain_data = downcast_dtypes(train_data)\ntest_data = downcast_dtypes(test_data)\nitems_data = downcast_dtypes(items_data)\nshops_data = downcast_dtypes(shops_data)\nitem_cat_data = downcast_dtypes(item_cat_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_dsi = ['date_block_num', 'shop_id', 'item_id']\ncol_si = ['shop_id', 'item_id']\ncol_di = ['date_block_num', 'item_id']\ncol_ds = ['date_block_num', 'shop_id']\ncol_d = ['date_block_num']\ncol_s = ['shop_id']\ncol_i = ['item_id']\ncol_dsicat = ['date_block_num', 'shop_id', 'item_category_id']\ncol_dicati = ['date_block_num', 'item_category_id', 'item_id']\ncol_dp = ['date_block_num', 'price_bins']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_itemscat = list (items_data.groupby ('item_category_id')['item_id'].count ().nlargest (15).index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fix category\nl_cat = list(item_cat_data.item_category_name)\nfor ind in range(0,1):\n    l_cat[ind] = 'PC Headsets / Headphones'\nfor ind in range(1,8):\n    l_cat[ind] = 'Access'\n    l_cat[8] = 'Tickets (figure)'\n    l_cat[9] = 'Delivery of goods'\nfor ind in range(10,18):\n    l_cat[ind] = 'Consoles'\nfor ind in range(18,25):\n    l_cat[ind] = 'Consoles Games'\n    l_cat[25] = 'Accessories for games'\nfor ind in range(26,28):\n    l_cat[ind] = 'phone games'\nfor ind in range(28,32):\n    l_cat[ind] = 'CD games'\nfor ind in range(32,37):\n    l_cat[ind] = 'Card'\nfor ind in range(37,43):\n    l_cat[ind] = 'Movie'\nfor ind in range(43,55):\n    l_cat[ind] = 'Books'\nfor ind in range(55,61):\n    l_cat[ind] = 'Music'\nfor ind in range(61,73):\n    l_cat[ind] = 'Gifts'\nfor ind in range(73,79):\n    l_cat[ind] = 'Soft'\nfor ind in range(79,81):\n    l_cat[ind] = 'Office'\nfor ind in range(81,83):\n    l_cat[ind] = 'Clean'\n    l_cat[83] = 'Elements of a food'\nitem_cat_data ['item_category_name'] = pd.DataFrame (l_cat)\n#items_data ['item_category_name'] = LabelEncoder ().fit_transform (items_data ['item_category_name'])\n\nitems_data = items_data.merge (item_cat_data, on = 'item_category_id', how = 'left').drop (['item_name'], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_data ['city'] = shops_data ['shop_name'].apply (lambda x : x.split (' ')[0])\nshops_data ['center'] = shops_data ['shop_name'].apply (lambda x : x.split (' ')[1])\nshops_data = shops_data.drop ('shop_name', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_data ['item_category_name'] = LabelEncoder ().fit_transform (items_data ['item_category_name']).astype (np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shops_data ['city'] = LabelEncoder ().fit_transform (shops_data ['city']).astype (np.int8)\nshops_data ['center'] = LabelEncoder ().fit_transform (shops_data ['center']).astype (np.int8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# checking data and converting to time format"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data ['date'] = pd.to_datetime (train_data ['date'], format = '%d.%m.%Y')\ntrain_data ['item_price'] = round (train_data ['item_price'],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rus_holidays = holidays.Russia ()\ntrain_data ['is_holiday'] = train_data ['date'].apply (lambda x : 1 if x in rus_holidays else 0)\nholiday_month = train_data [train_data.is_holiday == 1].groupby ('date_block_num')['date'].nunique ()\n\nholidays_df = pd.DataFrame (holiday_month).reset_index ()\nholidays_df.columns = ['date_block_num', 'holiday_count']\n\nmaster = pd.DataFrame (index = list (range (0,35)), columns = ['holidays']).reset_index ()\nmaster.columns = ['date_block_num', 'holidays']\nmaster ['holidays'] = 0\n\nholidays = master.merge (holidays_df, on = 'date_block_num', how = 'left').fillna (0).drop ('holidays', axis = 1)\n\nholidays ['date_block_num'] = holidays ['date_block_num'].astype (np.int8)\nholidays ['holiday_count'] = holidays ['holiday_count'].astype (np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (train_data.loc [train_data ['item_price']<0,:].shape [0])\nprint (train_data.loc [train_data ['item_cnt_day']<0,:].shape [0])\nprint (train_data ['item_price'].min ())\nprint (train_data ['item_cnt_day'].min ())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure (figsize = (15,5))\n\nfig.add_subplot (1,2,1)\ntrain_data[['item_cnt_day']].boxplot ()\n\nfig.add_subplot (1,2,2)\ntrain_data [['item_price']].boxplot ()\n\nplt.tight_layout ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.loc [(train_data ['item_price'] >= 0) & (train_data ['item_price'] < 300000) & (train_data ['item_cnt_day'] >=0) & (train_data ['item_cnt_day'] < 2000), :]\ntrain_data = train_data [['date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inactive test_data items\ntd_piv = train_data.pivot_table (index = 'item_id', columns = ['date_block_num'], values = 'item_cnt_day', aggfunc = 'sum').fillna (0)\nlast6 = td_piv.loc [:,td_piv.columns [27:33]]\ninactive_items = list (last6[last6.iloc [:, :].sum (axis = 1) == 0].index)\nactive_items = list (td_piv [td_piv.index.isin (inactive_items) == False].index)\nprint ('active items {}'.format (len (active_items)))\nprint ('Inactive items {}'.format (len (inactive_items)))\n\ninactive_test_items = list(set(test_data.loc [test_data ['item_id'].isin (inactive_items), 'item_id'].values))\nprint ('inactive_test_items {}'.format (len (inactive_test_items)))\n\nactive_test_items = list (set(test_data.loc [test_data ['item_id'].isin (inactive_test_items) == False, 'item_id'].values))\nprint ('active_test_items {}'.format (len (active_test_items)))\n\n# new test_data items\nnew_items = list(set (test_data.item_id)-set(train_data.item_id))\n\n# no forecast test_data items\nno_forecast_items = inactive_test_items + new_items\n\nprint ('no_forecast_items : {}'.format (len (no_forecast_items)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data ['revenue'] = train_data ['item_price'] * train_data ['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"revenue = train_data.groupby (col_dsi)['revenue'].sum ().reset_index ()\nrevenue.columns = col_dsi + ['revenue']\n\n\ntarget = train_data.groupby (col_dsi)['item_cnt_day'].sum ().reset_index ()\ntarget.columns = col_dsi + ['target']\n\nprice = train_data.groupby (col_dsi).agg ({'item_price' : ['mean']}).reset_index ().fillna (0)\nprice.columns = col_dsi + ['price']\n\ntrain_data = train_data [col_dsi].merge (target, on = col_dsi, how = 'left').fillna (0)\n\ntrain_data ['target'] = train_data ['target'].astype (np.float16)\n\ntrain_data = train_data.merge (price, on = col_dsi, how = 'left').fillna (0)\n\ntrain_data = train_data.merge (revenue, on = col_dsi, how = 'left').fillna (0)\n\ntrain_data = downcast_dtypes(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data ['date_block_num'] = train_data ['date_block_num'].astype (np.int8)\ntrain_data ['shop_id'] = train_data ['shop_id'].astype (np.int8)\ntrain_data ['price'] = train_data ['price'].astype (np.float16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import product\ngrid = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train_data[train_data.date_block_num==i]\n    grid.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \ngrid = pd.DataFrame(np.vstack(grid), columns=cols)\ngrid ['date_block_num'] = grid ['date_block_num'].astype(np.int8)\ngrid ['shop_id'] = grid ['shop_id'].astype(np.int8)\ngrid ['item_id'] = grid ['item_id'].astype(np.int16)\ngrid .sort_values(cols,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = grid.merge (train_data, on = col_dsi, how = 'left').drop_duplicates (subset = col_dsi, keep = 'first').fillna (0)\ntrain_data = train_data.sort_values (by = col_dsi, ascending = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### combining test-train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data ['date_block_num'] = 34\ntest_data ['date_block_num'] = test_data ['date_block_num'].astype (np.int8)\ntest_data ['shop_id'] = test_data ['shop_id'].astype (np.int8)\ntest_data ['item_id'] = test_data ['item_id'].astype (np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat ([train_data, test_data], axis = 0).fillna (0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### lags"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lags (train, lags, col):\n    col_dsi = ['date_block_num', 'shop_id', 'item_id']\n    df = train\n    \n    for i in lags:\n        prev = train [col_dsi + [col]].copy ()\n        \n        prev ['date_block_num'] = prev ['date_block_num'] + i\n        prev = prev.rename (columns = {col : 'prev_' + col + str (i)})\n        df = df.merge (prev, on = col_dsi, how = 'left')\n       \n    return df.fillna (0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lag features"},{"metadata":{"trusted":true},"cell_type":"code","source":"lags_ = [1,2,3,12]   \ndata = lags (data, lags_, 'target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lags_ = [1]   \ndata = lags (data, lags_, 'price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_mean = data.groupby ('item_id', as_index = False)['price'].max ()\ntarget_mean.columns = ['item_id'] + ['item_pricemax']\n\ndata = pd.merge (data, target_mean, on = ['item_id'], how = 'left')\n\ntarget_mean = data.groupby ('item_id', as_index = False)['price'].min ()\ntarget_mean.columns = ['item_id'] + ['item_pricemin']\n\ndata = pd.merge (data, target_mean, on = ['item_id'], how = 'left')\n\ndata ['discount'] = data ['item_pricemax'] - data ['item_pricemin']\n\n\nlags_ = [1]   \ndata = lags (data, lags_, 'discount')\ndata = data.drop ('discount', axis = 1)\n\ndata = data.drop (['item_pricemax','item_pricemin'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lags_ = [1]   \ndata = lags (data, lags_, 'revenue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data ['revenue'] = data ['revenue'].astype (np.float32)\ndata ['prev_revenue1'] = data ['prev_revenue1'].astype (np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_mean = data.groupby (col_di, as_index = False)['target'].mean ()\ntarget_mean.columns = col_di + ['target_meandi']\n\ndata = pd.merge (data, target_mean, on = col_di, how = 'left')\n\n\nlags_ = [1,2,3]   \ndata = lags (data, lags_, 'target_meandi')\ndata = data.drop ('target_meandi', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_mean = data.groupby (col_ds, as_index = False)['target'].mean ()\ntarget_mean.columns = col_ds + ['target_meands']\n\ndata = pd.merge (data, target_mean, on = col_ds, how = 'left')\n\nlags_ = [1]   \ndata = lags (data, lags_, 'target_meands')\ndata = data.drop ('target_meands', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_mean = data.groupby (col_ds, as_index = False)['revenue'].mean ()\ntarget_mean.columns = col_ds + ['revenue_means']\n\ndata = pd.merge (data, target_mean, on = col_ds, how = 'left')\n\nlags_ = [1]   \ndata = lags (data, lags_, 'revenue_means')\ndata = data.drop ('revenue_means', axis = 1)\n\ntarget_mean = data.groupby (col_s, as_index = False)['revenue'].mean ()\ntarget_mean.columns = col_s + ['revenue_means_tot']\n\ndata = pd.merge (data, target_mean, on = col_s, how = 'left')\n\nlags_ = [1]   \ndata = lags (data, lags_, 'revenue_means_tot')\ndata = data.drop ('revenue_means_tot', axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data ['revenue_delta'] = data ['prev_revenue_means_tot1'] - data ['prev_revenue_means1']\ndata = data.drop (['prev_revenue_means_tot1','prev_revenue_means1'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_mean = data.groupby (col_d, as_index = False)['target'].mean ()\ntarget_mean.columns = col_d + ['M_mean']\n\ndata = pd.merge (data, target_mean, on = col_d, how = 'left')\n\nlags_ = [1,2,3,12]\ndata = lags (data, lags_, 'M_mean') \ndata = data.drop ('M_mean', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.merge (shops_data, on = 'shop_id', how = 'left').fillna (0)\ndata = data.merge (items_data, on = 'item_id', how = 'left').fillna (0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_mean = data.groupby (col_dsicat, as_index = False)['target'].mean ()\ntarget_mean.columns = col_dsicat + ['target_meansicat']\n\ndata = pd.merge (data, target_mean, on = col_dsicat, how = 'left')\n\nlags_ = [1]   \ndata = lags (data, lags_, 'target_meansicat')\ndata = data.drop ('target_meansicat', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data ['inactive_items'] = data ['item_id'].isin (inactive_items).replace ({False:0, True:1}).astype (np.int16)\n\ndata = data.merge (holidays, on = 'date_block_num', how = 'left')\n\nbins = np.linspace (data.price.min (), data.price.max (),6)\nlabels = ['very_low', 'low', 'medium', 'high','very_high']\n\ndata ['price_bins'] = pd.cut(data ['price'],bins,labels = labels, include_lowest = True)\ndata ['price_bins'] = data ['price_bins'].astype ('O')\n\ndata ['price_bins'] = LabelEncoder ().fit_transform (data ['price_bins']).astype (np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data ['month'] = data ['date_block_num'].apply (lambda x : x%12+1).astype (np.int8)\n\ndata ['year'] = data ['date_block_num'].apply (lambda x : x//12).astype (np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\ndata['days'] = data['month'].map(days)\ndata['days'] = data['days'].astype (np.float16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# data for training model and validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"req_data = data.copy ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"req_data = req_data.drop (['item_category_id','price', 'revenue', 'shop_id', 'item_id'], axis = 1).fillna (0)\nreq_data = req_data.loc [req_data.date_block_num > 11, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndel data\ngc.collect()\ndel grid\ngc.collect()\ndel target_mean\ngc.collect()\ndel shops_data\ngc.collect()\ndel items_data\ngc.collect()\ndel target\ngc.collect()\ndel price\ngc.collect()\ndel train_data\ngc.collect ()\n\ndel item_cat_data\ngc.collect ()\ndel holidays\ngc.collect ()\ndel revenue\ngc.collect ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"req_data.info ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = req_data.loc [(req_data.date_block_num<33),:].drop (['date_block_num', 'target'], axis = 1)\n\ny = req_data.loc [(req_data.date_block_num<33),:]['target'].clip (0,20)\n\nX_valid = req_data [req_data ['date_block_num']==33].drop (['date_block_num', 'target'], axis = 1)\ny_valid = req_data [req_data ['date_block_num']==33]['target'].clip (0,20)\n\nX_test = req_data [req_data ['date_block_num']==34].drop (['date_block_num', 'target'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del req_data\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# checking feature correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"corrtest = X.copy ()\ncorrtest ['target'] = y\nplt.figure (figsize = (15,10))\nsns.heatmap (corrtest.corr (), annot = True, cmap = 'viridis', mask = corrtest.corr () < 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = XGBRegressor(\n    max_depth=9,\n    n_estimators=150,\n    min_child_weight=0.5, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.1,\n#     tree_method='gpu_hist',\n    seed=42, reg_lambda=1, gamma = 0)\n\nmodel.fit(\n    X, \n    y, \n    eval_metric=\"rmse\", \n    eval_set=[(X, y), (X_valid, y_valid)], \n    verbose=True, \n    early_stopping_rounds = 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction1 = model.predict (X_valid).clip (0,20)\nprediction2 = model.predict (X_test).clip (0,20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# checking "},{"metadata":{"trusted":true},"cell_type":"code","source":"Score = sqrt (mean_squared_error (prediction1, y_valid))\nprint (round (np.mean (Score),4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame (model.get_booster().get_score(importance_type='weight'), index =[0]).transpose ()\ndf.columns = ['features']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sort_values (by = 'features', ascending = True).plot.barh (figsize = (7,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter (prediction1, y_valid)\nplt.ylim ([0,25])\nplt.xlim ([0,25])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation = np.corrcoef (prediction1, y_valid)[0,1]\nround (correlation,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# outcome"},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'ID': submission_file.ID, 'item_cnt_month': prediction2})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}